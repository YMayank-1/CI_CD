name: Deploy

on:
  push:
    branches: [ main ]
    paths:
      - 'dags/**'
      - 'plugins/**'
      - 'sql/**'
      - 'json/**' 

  workflow_dispatch:

jobs:
  Deploy:
    runs-on: ubuntu-latest

    permissions:
      contents: 'read'
      actions: 'read'
      id-token: 'write'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: ${{ github.event_name == 'pull_request' && 2 || 0 }}

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: "${{ secrets.GOOGLE_CLOUD_KEY_PROD }}"

      - name: Setup Google Cloud SDK
        uses: google-github-actions/setup-gcloud@v1

      - name: Get gcloud info
        run: gcloud info

      - name: Dump GitHub context
        env:
          GITHUB_CONTEXT: ${{ toJson(github) }}
        run: echo "$GITHUB_CONTEXT"

      - name: Get changed files
        id: changed-files
        run: |
          if ${{ github.event_name == 'pull_request' }}; then
            echo "changed_files=$(git diff --name-only -r HEAD^1 HEAD ':!/.github/workflows/*' | xargs)" >> $GITHUB_OUTPUT
          else
            echo "changed_files=$(git diff --name-only ${{ github.event.before }} ${{ github.event.after }} ':!/.github/workflows/*' | xargs)" >> $GITHUB_OUTPUT
          fi

      - name: install python packages
        run: |
          cd cicd_scripts
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: List changed files
        run: |
          for file in ${{ steps.changed-files.outputs.changed_files }}; do
            echo "Changed File - $file"
          done

      - name: get working directory
        run: |
          pwd

      - name: Deploy Changes
        if: ${{ steps.changed-files.outputs.changed_files }}
        run: |
          for file in ${{ steps.changed-files.outputs.changed_files }}; do
            if [[ "$file" =~ ^(dags/|plugins/|sql/|json/) ]]; then
              echo "$file"
            
              # Check if the file exists
              if [[ -e "$file" ]]; then
                echo "Copying file to GCS Bucket - $file"

                echo gs://${{ vars.GCS_BUCKET_PROD }}/"$file"
                gsutil cp "$file" gs://${{ vars.GCS_BUCKET_PROD }}/"$file"

                # Check if the file is in the 'sql/' folder and has a '.sql' extension
                if [[ "$file" =~ ^sql/prod/bronze/.*\.sql$ ]]; then
                  echo "Executing Bronze Layer SQL file - $file"
                
                  #Pass project and secret names to python file args
                  python cicd_scripts/bq_sql_execution.py "$file" "${{ vars.BRONZE_PROJECT_ID_PROD }}" "${{ vars.BRONZE_SECRET_NAME_PROD }}" "${{ vars.BRONZE_PROJECT_ID_PROD }}"
                fi
                if [[ "$file" =~ ^sql/prod/silver/.*\.sql$ ]]; then
                  # Execute bq command to run the entire SQL file
                  echo "Executing Silver Layer SQL file - $file"

                  #Pass project and secret names to python file args
                  python cicd_scripts/bq_sql_execution.py "$file" "${{ vars.BRONZE_PROJECT_ID_PROD }}" "${{ vars.SILVER_SECRET_NAME_PROD }}" "${{ vars.SILVER_PROJECT_ID_PROD }}"
                fi
              
              else
                echo "Check file availability in GCS - $file"
                # remove it from the GCS bucket
                gsutil -q stat "gs://${{ vars.GCS_BUCKET_PROD }}/$file" 
                status=$?
                if [[ $status == 0 ]]; then
                  echo "File exists"
                  gsutil rm "gs://${{ vars.GCS_BUCKET_PROD }}/$file"
                else
                  echo "File does not exist, Skipping the deletion"  
                  echo  "gs://${{ vars.GCS_BUCKET_PROD }}/$file"
                fi
              fi
            fi
          done

      - name: Check for new JSON files
        id: new-json-files
        run: |
          if [[ -n "$(git diff --name-only HEAD^ HEAD 'json/*.json')" ]]; then
            echo "::set-output name=has_new_json::true"
          else
            echo "::set-output name=has_new_json::false"
          fi
        continue-on-error: true

      - name: Create BigQuery table from JSON
        if: ${{ steps.new-json-files.outputs.has_new_json }}
        run: |
          # Parse JSON file content
          json_file=$(ls -t json/*.json | head -n 1)
          json_file_content=$(cat "$json_file")
          project_id=$(echo "$json_file_content" | jq -r '.project_id')
          dataset_id=$(echo "$json_file_content" | jq -r '.dataset_id')
          

          table_name=$(basename json/json_file.json)
          schema=$(echo "$json_file_content" | jq -r '.schema')
         
          partitions=$(echo "$json_file_content" | jq -r '.partitions | join(",")')

          # Create BigQuery table with partitioning
          bq mk --table --project_id="$project_id" --dataset_id="$dataset_id" --table="$table_name" --schema="$schema"  --time_partitioning_type=DAY --time_partitioning_field="$partitions"

      - name: notify slack
        uses: 8398a7/action-slack@v3
        with:
          author_name: 'Production Deployment'
          status: ${{ job.status }}
          fields: repo,author,job,took
        if: always()
